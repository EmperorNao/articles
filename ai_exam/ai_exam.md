
# Экзамен по "Основы ИИ"


# Список тем

1. История развития ИИ
2. Алгоритм отжига
3. Теория адаптивного резонанса: алгоритмы кластеризации. ART-1
4. Муравьиный алгоритм
5. Нейронные сети
6. Генетический алгоритм
7. Продукционная модель
8. Фреймы
9. Задачи


## 1. История развития ИИ

Искусственный интеллект (ИИ) – это отрасль науки, официально увидевшая свет в 1956 году на летнем семинаре в Дартмут-колледже (Хановер, США), который организовали четверо американских ученых: Джон Мак-Карти, Марвин Мински, Натаниэль Рочестер и Клод Шеннон.

ИИ изначально представлял собой область науки, занимающейся компьютерным моделированием различных способностей интеллекта, идет ли речь об интеллекте человеческом, животном, растительном, социальном или филогенетическом. В основе этой научной дисциплины лежит предположение о том, что все когнитивные функции, как то обучение, мышление, расчет, восприятие, память, даже научное открытие или художественное творчество, могут быть описаны с точностью, дающей возможность запрограммировать компьютер на их воспроизведение.

1. Период пророчеств

    Поначалу, под влиянием первых успехов, исследователи позволяли себе несколько опрометчивые заявления, которые впоследствии неоднократно ставились им в упрек. Так, например, в 1958 году американец Герберт Саймон, позже ставший лауреатом Нобелевской премии по экономике, заявил, что если бы машины допускались к международным соревнованиям, то в ближайшие десять лет они стали бы чемпионами мира по шахматам. 

2. Мрачные времена

    Прогресс замедлился в середине 1960-х годов. В 1966 году в докладе, подготовленном по заказу Сената Соединенных Штатов Америки, говорилось о внутренних ограничениях, присущих машинному переводу. Около десяти лет пресса отзывалась об ИИ неодобрительно.

3. Семантический ИИ

    Исследования не прекратились, но пошли в новых направлениях. Ученые заинтересовались психологией памяти, механизмами понимания, которые они пытались имитировать на компьютере, и ролью знаний в мыслительном процессе. Это привело к появлению значительно развившихся в середине 1970-х годов методов семантического представления знаний, а также к созданию экспертных систем, названных так потому, что для воспроизведения мыслительных процессов в них использовались знания квалифицированных специалистов. В начале 1980-х годов на экспертные системы возлагались большие надежды в связи с широкими возможностями их применения, например, для медицинской диагностики.

4. Неоконнекционизм и машинное обучение

    Технические усовершенствования позволили разработать алгоритмы машинного обучения (Machine Learning), благодаря которым компьютеры смогли накапливать знания и автоматически перепрограммироваться на основе собственного опыта.

5. От ИИ до интерфейсов «человек – машина»

    С конца 1990-х годов ИИ стали объединять с робототехникой и интерфейсом «человек – машина» с целью создания интеллектуальных агентов, предполагающих наличие чувств и эмоций. Это привело, среди прочего, к появлению нового исследовательского направления – аффективных (или эмоциональных) вычислений (affective computing), направленных на анализ реакций субъекта, ощущающего эмоции, и их воспроизведение на машине, и позволило усовершенствовать диалоговые системы (чат-боты).

6. Возрождение ИИ

    С 2010 года мощность компьютеров позволяет сочетать так называемые большие данные (Big Data) с методами глубокого обучения (Deep Learning), которые основываются на использовании искусственных нейронных сетей. Весьма успешное применение во многих областях (распознавание речи и изображений, понимание естественного языка, беспилотный автомобиль и т.д.) позволяет говорить о возрождении ИИ.

 

## 2. Алгоритм отжига

Оптимизационный алгоритм. Заключается в итеративном поиске лучших параметров через случайное блуждание в сужающемся (относительно вероятности) пространстве, являющемся подпространством исходного решения.

Генерируем начальное решение и оцениваем его качество - назовём эту оценку качества энергией. Наилучшим решением будет решение с наименьшей энергией.

Также, инициализируем параметры температуры - которым определяется вероятностное подпространство в пространстве всех исходных решений.
Чем больше параметр температуры T - тем больше пространство поиска нового решения.

Внутри одной итерации, будем k раз повторять следующую последовательность действий: 

1. Случайным образом изменить текущее решение (желательно)
2. Оценка энергии 
3. Если новое решение имеет меньшую энергию E чем текущую, то заменим его. Если же оно имеет большую энергию, то проверим его по критерию допуска. 
Сгенерируем случайную переменную 0 <= p_g <= 1. Если данное p_g меньше чем условие допуска p(deltaЕ) = exp(-deltaE/T), то заменим текущее решение даже если его энергия больше.
Будем также запоминать лучшее полученное решение.


Таким образом, внутри итераций мы пытаемся найти какое-то лучшее решение не только из жадных с помощью случайных блужданий.

После каждой итерации будем уменьшать температуру по заданному правилу. Будем продолжать итерации до тех пор, пока лучшая энергия не станет меньше или равной минимальной, либо пока температура не станет меньше минимальной.

Основные параметры - количество внутренних итераций, способ уменьшения температуры, минимальная температура.



## 3. Теория адаптивного резонанса: алгоритмы кластеризации. ART-1

Основная идея заключается в том, что распознавание образов является результатом нисходящих ожиданий и восходящей сенсорной информации. Причем нисходящие ожидания принимают форму припоминаемых прототипов или образцов, которые затем сравниваются с реально наблюдаемыми свойствами объекта. Это сравнение лежит в основании меры категориальной принадлежности. Когда разница между ожиданием и наблюдаемым не превышает определенный порог («бдительность») наблюдаемый объект считается принадлежащим к определенной категории. Таким образом система предлагает решение проблемы пластичности/стабильности, то есть проблемы приобретения нового знания без нарушения уже существующего.

ART-1 является алгоритмом кластеризации и работает только с объектами с бинарным описанием.

На входе принимает вектора объектов для кластеризации.

Параметры:
- N - максимальное количество прототипов.
- d - размер вектора
- b - небольшое число
- p - параметр внимания (принадлежит от 0 до 1)


Будем по очереди определять каждый вектор к одному из заданных прототипов.

Принадлежность к кластеру определяется следующим правилом (P - прототип, E - объект):

|P and E| / (b + |P|) > |E| / (b + d)

Если принадлежность кластеру пройдена, то далее проходится тест на внимательность:

|P and E| / |E| > p

При прохождении всех тестов, вектор прототип кластера обновляется по правилу 

P = P and E

и вектор добавляется в заданный кластер. Если же для данного вектора не находится подходящего кластера, то создадим новый на его основе.


## 4. Муравьиный алгоритм

Оптимизационный алгоритм, основанный на физическом поведении муравьёв.

Муравьиный алгоритм хорошо подходит для алгоритмов поиска оптимального пути.

В каждый момент перехода по графу, муравей выбирает с некой вероятностью одну из следующих вершин, которую он не посетил.
Эта вероятность выражается как p = t(r, u)^a * h(r, u)^b / sum_near_v(t(r, near_v)^a * h(r, near_v)^b)

t(r, u) - интенсивность фермента на ребре, h(r, u) - длина ребра.
a - вес фермента, b - вес длины ребра.

Посетив все вершины по одной, муравей узнаёт длину своего пути. На каждой грани было оставлено D_ij = Q/L фермента.

Q - константа, L - длина пути.

Обновление ферментов происходит по формуле 

t_ij = D_ij + r * t_IJ

r - ещё один параметр, принадлежит от 0 до 1.

Для удаление неэффективных путей - используется испарение фермента. 

t_ij = t_ij * (1 - r).


Алгоритм применяется для нескольких муравьёв одновременно. Затем они обновляют ферменты, происходит испарение.
Алгоритм повторяется до некого условия остановки. 

Затем определяется лучший путь.


## 5. Нейронные сети

### ИНС
ИНС - произведение матриц с последовательным применением функций активации.

### Однослойный персептрон
Однослойный персептрон - на входе вектор x размерности 1xN. Обладаем матрицей весов W размера 1xN.
Производим операцию <W, x^T> скалярного произведения (сумматор), далее к полученному числу применяем функцию активации F (обычно нелинейную).


### Обучение нейрона по правилу Хебба
Первое правило Хебба — Если сигнал персептрона неверен и равен нулю, то необходимо увеличить веса тех входов, на которые была подана единица.

Второе правило Хебба — Если сигнал персептрона неверен и равен единице, то необходимо уменьшить веса тех входов, на которые была подана единица.


## 6. Генетический алгоритм




## 7. Продукционная модель


## 8. Фреймы


## 9. Задачи

- [Дана начальная популяция из четырех хромосом с двумя генами x и y. Найти максимальный показатель качества хромосомы в популяции и общее качество популяции после четырех этапов эволюции](http://vuz.exponenta.ru/PDF/book/bm63.pdf)

- Дана многослойная нейронная сеть, матрицы весовых коэффициентов связей

  вход: X (1x4), Y (1x2)

  веса: W1 (4x3), W2 (3, 2)

  активации: f1, f2

  лосс: L(X, Y)

  скорость обучения: lr

  forward: 
  ```
  A0 = X^T
  X1 = W1^T x A0   | (3, 1)
  A1 = f1(X1)   | (3, 1)
  X2 = W2^T x A1   | (2, 1)
  A2 = f2(X2)   | (2, 1)
  loss = L(A2, Y)   | (1)
  ```
  
  backward:
  ```
  E2 = dloss/dW2 = dloss/dA2 * df2/dX2   | (2, 1)
  E1 = dloss/dW1 = (W2 x E2) * df1/dX1   | (3, 1)
  
  dloss/dW2 = A1 x E2^T   | (3, 2)
  dloss/dW1 = A0 x E1^T   | (4, 3)

  W2 = W2 - lr * dloss/dW2
  W1 = W1 - lr * dloss/dW1
  ```

- [Найти наилучшее размещение графа на линейке после трех циклов генетического алгоритма](http://vuz.exponenta.ru/PDF/book/nnet72.pdf)

- [Найти длину гамильтонова цикла S в полном графе после четырех циклов решения задачи методом отжига]((http://vuz.exponenta.ru/PDF/book/bm61.pdf))


  
