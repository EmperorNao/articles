## CRINGE loss
[source](https://arxiv.org/abs/2211.05826)


Идея - один из способов обучаться на негативных примерах (negative mining).
В статье говорится про генерацию текстов.

Хотим избегать:
 - токсичность и биас
 - ложь и противоречие
 - генерим что-то адекватное, но неподходящее по контексту промпта
 - повторения
 
Обычно в датасетах есть много positive examples, но мало negative examples. Всё чаще и чаще появляются такие датасеты.

Positive: учимся на CELoss

Negative: непонятно

Способы:
1. бейзлайн: убираем все негативы
2. unlikelihood training: ULLoss, снижаем вероятность повторов регуляризацией
   минус -> повышаем вероятность всех остальных плохих токенов помимо повтора
3. CELoss при подстановке логитов -> похоже на Contrastive loss
   Начинаем использовать Contrastive token learning, ходим CELoss'ом по последним M негативам
4. CRINGE Loss -> майним позитивы. Модель предиктит topk, делаем софтмакс семпл, используем его как позитив.
   log(1 + exp(s_xt - s_+)). Сохраняем, что модель бы и так предсказала, но понижаем относ. вероятность негатива.
